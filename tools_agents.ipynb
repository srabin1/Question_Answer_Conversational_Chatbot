{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Engine With Tools And Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs cannot directly:\n",
    "\n",
    "Call HTTP APIs\n",
    "\n",
    "Parse unpredictable API responses\n",
    "\n",
    "Handle rate limits or parameters safely\n",
    "\n",
    "Wrappers solve this by:\n",
    "\n",
    "Exposing simple Python methods\n",
    "\n",
    "Returning clean text the LLM can reason over\n",
    "\n",
    "Making tools agent-friendly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Arxiv--Research\n",
    "## Tools creation\n",
    "from langchain_community.tools import ArxivQueryRun,WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper,ArxivAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the inbuilt tool of wikipedia\n",
    "api_wrapper_wiki = WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=250)\n",
    "# then run the wrapper\n",
    "wiki = WikipediaQueryRun(api_wrapper = api_wrapper_wiki)\n",
    "wiki.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_wrapper_arxiv = ArxivAPIWrapper(top_k_results = 1, doc_content_chars_max=250)\n",
    "arxiv = ArxivQueryRun(api_wrapper=api_wrapper_arxiv)\n",
    "arxiv.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [wiki, arxiv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sanaz.rabinia\\Gen_AI_Udemy\\python\\Question_Answer_Chatbot\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Custom tools [RAG Tools]\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000027EDCB4B730>, search_kwargs={})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "docs=loader.load()\n",
    "documents=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200).split_documents(docs)\n",
    "vectordb=FAISS.from_documents(documents,OpenAIEmbeddings())\n",
    "retriever=vectordb.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the retriever into tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langsmith-search'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.tools.retriever import create_retriever_tool\n",
    "retriever_tool=create_retriever_tool(retriever,\"langsmith-search\",\"Search any information about Langsmith \")\n",
    "\n",
    "retriever_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[wiki,arxiv,retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'c:\\\\Users\\\\sanaz.rabinia\\\\Gen_AI_Udemy\\\\python\\\\Question_Answer_Chatbot\\\\venv\\\\lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=250)),\n",
       " ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=250)),\n",
       " StructuredTool(name='langsmith-search', description='Search any information about Langsmith ', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=<function create_retriever_tool.<locals>.func at 0x0000027EDCB8B7F0>, coroutine=<function create_retriever_tool.<locals>.afunc at 0x0000027EDCA8E680>)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run all this tools with Agents and LLM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tools, LLM-->AgentExecutor\n",
    "import openai\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\") \n",
    "openai.api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"openai/gpt-oss-120b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_classic import hub\n",
    "prompt=hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000027EBBCB6EF0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000027EBBCB6EF0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 32768, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x0000027EF5CAE230>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000027EF5CAE920>, model_name='openai/gpt-oss-120b', model_kwargs={}, groq_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'langsmith-search', 'description': 'Search any information about Langsmith ', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| OpenAIToolsAgentOutputParser()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is one of the ways to create agents\n",
    "from langchain_classic.agents import create_openai_tools_agent\n",
    "agent=create_openai_tools_agent(llm,tools,prompt)\n",
    "agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent Executer to run the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentExecutor(verbose=True, agent=RunnableMultiActionAgent(runnable=RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000027EBBCB6EF0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000027EBBCB6EF0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 32768, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x0000027EF5CAE230>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000027EF5CAE920>, model_name='openai/gpt-oss-120b', model_kwargs={}, groq_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'langsmith-search', 'description': 'Search any information about Langsmith ', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| OpenAIToolsAgentOutputParser(), input_keys_arg=[], return_keys_arg=[], stream_runnable=True), tools=[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'c:\\\\Users\\\\sanaz.rabinia\\\\Gen_AI_Udemy\\\\python\\\\Question_Answer_Chatbot\\\\venv\\\\lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=250)), ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=250)), StructuredTool(name='langsmith-search', description='Search any information about Langsmith ', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=<function create_retriever_tool.<locals>.func at 0x0000027EDCB8B7F0>, coroutine=<function create_retriever_tool.<locals>.afunc at 0x0000027EDCA8E680>)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_classic.agents import AgentExecutor\n",
    "agent_executor=AgentExecutor(agent=agent,tools=tools,verbose=True)\n",
    "agent_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `langsmith-search` with `{'query': 'Langsmith'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mLangSmith is framework agnostic, so you can use it with or without LangChain’s open-source libraries\n",
      "langchain and langgraph.\n",
      "Prototype locally, then move to production with integrated monitoring and evaluation to build more reliable AI systems.\n",
      "LangGraph Platform is now LangSmith Deployment. For more information, check out the Changelog.\n",
      "​Get started\n",
      "Create an accountSign up at smith.langchain.com (no credit card required).\n",
      "You can log in with Google, GitHub, or email.Create an API keyGo to your Settings page → API Keys → Create API Key.\n",
      "Copy the key and save it securely.\n",
      "Once your account and API key are ready, choose a quickstart to begin building with LangSmith:\n",
      "\n",
      "LangSmith docs - Docs by LangChainSkip to main contentDocs by LangChain home pageLangSmithSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangSmith docsGet startedObservabilityEvaluationPrompt engineeringDeploymentAgent BuilderPlatform setupReferenceOverviewPlansCreate an account and API keyAccount administrationOverviewSet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementAdditional resourcesPolly (Beta)Data managementAccess control & AuthenticationScalability & resilienceFAQsRegions FAQPricing FAQLangSmith statusLangSmith docsCopy pageCopy pageLangSmith provides tools for developing, debugging, and deploying LLM applications.\n",
      "It helps you trace requests, evaluate outputs, test prompts, and manage deployments in one place.\n",
      "LangSmith is framework agnostic, so you can use it with or without LangChain’s open-source libraries\n",
      "langchain and langgraph.\n",
      "\n",
      "​More ways to build\n",
      "Platform setupUse LangSmith in managed cloud, in a self-hosted environment, or hybrid to match your infrastructure and compliance needs.Choose how to set up LangSmithAgent Builder (Beta)Design and deploy AI agents visually with a no-code interface—perfect for rapid prototyping and getting started without writing code.Build an agentStudioUse a visual interface to design, test, and refine applications end-to-end.Develop with StudioPrompt testingIterate on prompts with built-in versioning and collaboration to ship improvements faster.Test your prompts\n",
      "LangSmith meets the highest standards of data security and privacy with HIPAA, SOC 2 Type 2, and GDPR compliance. For more information, see the Trust Center.\n",
      "​Workflow\n",
      "LangSmith combines observability, evaluation, deployment, and platform setup in one integrated workflow—from local development to production.\n",
      "\n",
      "Edit this page on GitHub or file an issue.\n",
      "Connect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoPricing plansNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify\u001b[0m\u001b[32;1m\u001b[1;3m**LangSmith – Overview**\n",
      "\n",
      "LangSmith is a **full‑stack observability, evaluation, and deployment platform for building and running LLM‑powered applications**. It is created by the team behind **LangChain** (and its newer graph‑based counterpart LangGraph), but it is **framework‑agnostic**—you can use it with LangChain, LangGraph, or any other LLM‑orchestration library.\n",
      "\n",
      "---\n",
      "\n",
      "### Core Capabilities\n",
      "\n",
      "| Area | What LangSmith Provides |\n",
      "|------|--------------------------|\n",
      "| **Observability** | End‑to‑end tracing of every request that hits your LLM pipeline (inputs, outputs, intermediate steps, latency, token usage, etc.). |\n",
      "| **Evaluation** | Built‑in metrics and custom evaluation functions to score responses (e.g., factuality, relevance, safety). You can run batch evaluations and see visual dashboards. |\n",
      "| **Prompt Engineering** | Version‑controlled prompt editing, collaborative review, and “prompt testing” that lets you iterate quickly and compare revisions side‑by‑side. |\n",
      "| **Deployment** | One‑click deployment of agents or chain components to a managed cloud, a self‑hosted environment, or a hybrid setup. |\n",
      "| **Platform Setup** | Workspace‑level resource tagging, organization management, API‑key control, and billing dashboards. |\n",
      "| **Agent Builder (Beta)** | No‑code visual editor for designing AI agents—drag‑and‑drop nodes, define tool calls, and deploy without writing code. |\n",
      "| **Studio** | A visual IDE for end‑to‑end development: design, test, debug, and refine LLM applications in a single UI. |\n",
      "| **Security & Compliance** | HIPAA, SOC 2 Type 2, and GDPR compliance; data is encrypted at rest and in transit. |\n",
      "| **Integrations** | VS Code, Claude, GitHub, and other developer tools via the “MCP” (Multipurpose Connectors) system. |\n",
      "\n",
      "---\n",
      "\n",
      "### Typical Workflow\n",
      "\n",
      "1. **Prototype locally** – Write your chain/agent in your favorite IDE, run it, and let LangSmith automatically capture traces.\n",
      "2. **Debug & Test** – Use the trace UI to inspect each step, replay runs, and run prompt‑testing experiments.\n",
      "3. **Evaluate** – Apply built‑in or custom evaluators to batch‑run results and monitor key performance indicators.\n",
      "4. **Iterate** – Update prompts, chain logic, or model parameters; LangSmith tracks versions so you can compare before/after.\n",
      "5. **Deploy** – Push the vetted version to a managed LangSmith deployment, a self‑hosted instance, or a hybrid environment.\n",
      "6. **Monitor in Production** – Continuous observability dashboards keep you aware of drift, latency spikes, or cost overruns.\n",
      "\n",
      "---\n",
      "\n",
      "### Getting Started\n",
      "\n",
      "1. **Create an account** at <https://smith.langchain.com> (no credit‑card required). Sign‑in with Google, GitHub, or email.  \n",
      "2. **Generate an API key** (Settings → API Keys → “Create API Key”). Keep it secret.  \n",
      "3. **Pick a quick‑start** – LangSmith ships starter templates for:\n",
      "   * Plain LangChain/LangGraph projects  \n",
      "   * Generic HTTP‑based LLM pipelines  \n",
      "   * No‑code Agent Builder or Studio projects  \n",
      "4. **Instrument your code** – Add a few lines (or use the LangChain integration) to send traces to LangSmith.  \n",
      "5. **Explore the UI** – View traces, run evaluations, and experiment with prompt versions.\n",
      "\n",
      "---\n",
      "\n",
      "### Pricing (as of 2024)\n",
      "\n",
      "| Plan | Main Features | Approx. Cost |\n",
      "|------|----------------|--------------|\n",
      "| **Free** | Up to 10 k traces/month, basic dashboards, community support | $0 |\n",
      "| **Pro** | Unlimited traces, advanced evaluation, private workspaces, SLA support | $49 / month per user |\n",
      "| **Enterprise** | Dedicated instance (self‑hosted or private cloud), SSO, custom compliance, priority support | Custom pricing |\n",
      "\n",
      "*(Exact numbers may change; see the LangSmith pricing page for the latest details.)*\n",
      "\n",
      "---\n",
      "\n",
      "### Who Should Use LangSmith?\n",
      "\n",
      "| Audience | Why It Helps |\n",
      "|----------|--------------|\n",
      "| **LLM developers & data scientists** | Centralized debugging & evaluation, reduces time spent hunting bugs in chain logic. |\n",
      "| **Product teams building AI features** | Collaboration tools for prompt versioning and shared dashboards. |\n",
      "| **Enterprises with compliance needs** | HIPAA/GDPR‑ready environment, audit logs, role‑based access. |\n",
      "| **Start‑ups looking to ship quickly** | No‑code Agent Builder + one‑click deployment accelerates MVP cycles. |\n",
      "\n",
      "---\n",
      "\n",
      "### Quick Comparison with Alternatives\n",
      "\n",
      "| Feature | LangSmith | Weights & Biases (MLflow) | PromptLayer |\n",
      "|---------|-----------|---------------------------|-------------|\n",
      "| **LLM‑specific tracing** | ✅ (native chain/agent support) | ❌ (general ML) | ✅ (prompt‑only) |\n",
      "| **Prompt versioning & testing** | ✅ (built‑in UI) | ❌ (requires custom code) | ✅ |\n",
      "| **Agent visual builder** | ✅ (Beta) | ❌ | ❌ |\n",
      "| **Self‑hosted option** | ✅ (Hybrid/On‑prem) | ✅ | ❌ |\n",
      "| **Compliance certifications** | HIPAA, SOC 2, GDPR | SOC 2 (no HIPAA) | None |\n",
      "\n",
      "---\n",
      "\n",
      "### TL;DR\n",
      "\n",
      "- **LangSmith** is a **comprehensive platform for building, debugging, evaluating, and deploying LLM applications**.\n",
      "- It works with any LLM framework (LangChain, LangGraph, or custom code) and offers **observability, prompt‑testing, evaluation, and deployment** in one place.\n",
      "- **Free tier** lets you try it immediately; paid tiers add unlimited traces, advanced security, and enterprise‑grade support.\n",
      "- Ideal for anyone who wants **reliable, production‑ready AI systems** without stitching together separate monitoring, testing, and deployment tools.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Tell me about Langsmith',\n",
       " 'output': '**LangSmith – Overview**\\n\\nLangSmith is a **full‑stack observability, evaluation, and deployment platform for building and running LLM‑powered applications**. It is created by the team behind **LangChain** (and its newer graph‑based counterpart LangGraph), but it is **framework‑agnostic**—you can use it with LangChain, LangGraph, or any other LLM‑orchestration library.\\n\\n---\\n\\n### Core Capabilities\\n\\n| Area | What LangSmith Provides |\\n|------|--------------------------|\\n| **Observability** | End‑to‑end tracing of every request that hits your LLM pipeline (inputs, outputs, intermediate steps, latency, token usage, etc.). |\\n| **Evaluation** | Built‑in metrics and custom evaluation functions to score responses (e.g., factuality, relevance, safety). You can run batch evaluations and see visual dashboards. |\\n| **Prompt Engineering** | Version‑controlled prompt editing, collaborative review, and “prompt testing” that lets you iterate quickly and compare revisions side‑by‑side. |\\n| **Deployment** | One‑click deployment of agents or chain components to a managed cloud, a self‑hosted environment, or a hybrid setup. |\\n| **Platform Setup** | Workspace‑level resource tagging, organization management, API‑key control, and billing dashboards. |\\n| **Agent Builder (Beta)** | No‑code visual editor for designing AI agents—drag‑and‑drop nodes, define tool calls, and deploy without writing code. |\\n| **Studio** | A visual IDE for end‑to‑end development: design, test, debug, and refine LLM applications in a single UI. |\\n| **Security & Compliance** | HIPAA, SOC\\u202f2\\u202fType\\u202f2, and GDPR compliance; data is encrypted at rest and in transit. |\\n| **Integrations** | VS\\u202fCode, Claude, GitHub, and other developer tools via the “MCP” (Multipurpose Connectors) system. |\\n\\n---\\n\\n### Typical Workflow\\n\\n1. **Prototype locally** – Write your chain/agent in your favorite IDE, run it, and let LangSmith automatically capture traces.\\n2. **Debug & Test** – Use the trace UI to inspect each step, replay runs, and run prompt‑testing experiments.\\n3. **Evaluate** – Apply built‑in or custom evaluators to batch‑run results and monitor key performance indicators.\\n4. **Iterate** – Update prompts, chain logic, or model parameters; LangSmith tracks versions so you can compare before/after.\\n5. **Deploy** – Push the vetted version to a managed LangSmith deployment, a self‑hosted instance, or a hybrid environment.\\n6. **Monitor in Production** – Continuous observability dashboards keep you aware of drift, latency spikes, or cost overruns.\\n\\n---\\n\\n### Getting Started\\n\\n1. **Create an account** at <https://smith.langchain.com> (no credit‑card required). Sign‑in with Google, GitHub, or email.  \\n2. **Generate an API key** (Settings → API Keys → “Create API Key”). Keep it secret.  \\n3. **Pick a quick‑start** – LangSmith ships starter templates for:\\n   * Plain LangChain/LangGraph projects  \\n   * Generic HTTP‑based LLM pipelines  \\n   * No‑code Agent Builder or Studio projects  \\n4. **Instrument your code** – Add a few lines (or use the LangChain integration) to send traces to LangSmith.  \\n5. **Explore the UI** – View traces, run evaluations, and experiment with prompt versions.\\n\\n---\\n\\n### Pricing (as of 2024)\\n\\n| Plan | Main Features | Approx. Cost |\\n|------|----------------|--------------|\\n| **Free** | Up to 10\\u202fk traces/month, basic dashboards, community support | $0 |\\n| **Pro** | Unlimited traces, advanced evaluation, private workspaces, SLA support | $49\\u202f/\\u202fmonth per user |\\n| **Enterprise** | Dedicated instance (self‑hosted or private cloud), SSO, custom compliance, priority support | Custom pricing |\\n\\n*(Exact numbers may change; see the LangSmith pricing page for the latest details.)*\\n\\n---\\n\\n### Who Should Use LangSmith?\\n\\n| Audience | Why It Helps |\\n|----------|--------------|\\n| **LLM developers & data scientists** | Centralized debugging & evaluation, reduces time spent hunting bugs in chain logic. |\\n| **Product teams building AI features** | Collaboration tools for prompt versioning and shared dashboards. |\\n| **Enterprises with compliance needs** | HIPAA/GDPR‑ready environment, audit logs, role‑based access. |\\n| **Start‑ups looking to ship quickly** | No‑code Agent Builder + one‑click deployment accelerates MVP cycles. |\\n\\n---\\n\\n### Quick Comparison with Alternatives\\n\\n| Feature | LangSmith | Weights & Biases (MLflow) | PromptLayer |\\n|---------|-----------|---------------------------|-------------|\\n| **LLM‑specific tracing** | ✅ (native chain/agent support) | ❌ (general ML) | ✅ (prompt‑only) |\\n| **Prompt versioning & testing** | ✅ (built‑in UI) | ❌ (requires custom code) | ✅ |\\n| **Agent visual builder** | ✅ (Beta) | ❌ | ❌ |\\n| **Self‑hosted option** | ✅ (Hybrid/On‑prem) | ✅ | ❌ |\\n| **Compliance certifications** | HIPAA, SOC\\u202f2, GDPR | SOC\\u202f2 (no HIPAA) | None |\\n\\n---\\n\\n### TL;DR\\n\\n- **LangSmith** is a **comprehensive platform for building, debugging, evaluating, and deploying LLM applications**.\\n- It works with any LLM framework (LangChain, LangGraph, or custom code) and offers **observability, prompt‑testing, evaluation, and deployment** in one place.\\n- **Free tier** lets you try it immediately; paid tiers add unlimited traces, advanced security, and enterprise‑grade support.\\n- Ideal for anyone who wants **reliable, production‑ready AI systems** without stitching together separate monitoring, testing, and deployment tools.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"Tell me about Langsmith\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m**Machine learning (ML)** is a sub‑field of artificial intelligence (AI) that focuses on developing algorithms and statistical models that enable computers to *learn* from data, rather than being explicitly programmed for every task. In other words, instead of writing a set of hard‑coded rules, we give a system examples (training data) and let it infer the underlying patterns, relationships, or decision boundaries that it can then apply to new, unseen data.\n",
      "\n",
      "---\n",
      "\n",
      "## Core Idea\n",
      "\n",
      "1. **Data → Model → Prediction**  \n",
      "   - **Data**: A collection of examples (e.g., images, text, sensor readings) often accompanied by labels (e.g., “cat” vs. “dog”).  \n",
      "   - **Model**: A parameterized mathematical function (e.g., a linear equation, a neural network) that maps inputs to outputs.  \n",
      "   - **Learning**: An optimization process that adjusts the model’s parameters to minimize a loss (error) on the training data.  \n",
      "   - **Prediction/Inference**: Once trained, the model can generate outputs for new inputs.\n",
      "\n",
      "2. **Generalization**  \n",
      "   The ultimate goal is *generalization*: the ability of the learned model to perform well on data it has never seen before.\n",
      "\n",
      "---\n",
      "\n",
      "## Main Types of Machine Learning\n",
      "\n",
      "| Category | How it works | Typical tasks | Example algorithms |\n",
      "|----------|--------------|---------------|--------------------|\n",
      "| **Supervised learning** | Learns a mapping from inputs **x** to known outputs **y** using labeled examples. | Classification, regression, object detection | Linear regression, logistic regression, decision trees, support‑vector machines, neural networks |\n",
      "| **Unsupervised learning** | Discovers structure in data without explicit labels. | Clustering, dimensionality reduction, anomaly detection | k‑means, hierarchical clustering, Gaussian mixture models, PCA, autoencoders |\n",
      "| **Semi‑supervised learning** | Combines a small set of labeled data with a larger set of unlabeled data. | Improves performance when labeling is expensive | Self‑training, graph‑based methods, consistency regularization |\n",
      "| **Self‑supervised learning** | Generates pseudo‑labels from the data itself (e.g., predicting missing parts). | Pre‑training representations for downstream tasks | Contrastive learning, masked language modeling (BERT) |\n",
      "| **Reinforcement learning (RL)** | An agent learns by interacting with an environment and receiving reward signals. | Game playing, robotics, resource allocation | Q‑learning, policy gradients, Deep Q‑Network (DQN), Proximal Policy Optimization (PPO) |\n",
      "| **Online / incremental learning** | Updates the model continuously as new data arrives. | Real‑time recommendation, streaming analytics | Stochastic gradient descent, online bagging |\n",
      "| **Transfer learning** | Reuses knowledge from one task/domain to accelerate learning on another. | Fine‑tuning a pretrained image classifier on a new dataset | Pre‑trained CNNs (e.g., ResNet), language models (e.g., GPT) |\n",
      "\n",
      "---\n",
      "\n",
      "## Key Concepts & Terminology\n",
      "\n",
      "- **Feature**: A measurable property or representation of the input (e.g., pixel intensity, word frequency).  \n",
      "- **Label / Target**: The ground‑truth output the model should predict (e.g., “spam” vs. “not spam”).  \n",
      "- **Loss function**: Quantifies how far the model’s predictions are from the true labels (e.g., mean squared error, cross‑entropy).  \n",
      "- **Optimization algorithm**: Adjusts model parameters to minimize the loss (e.g., gradient descent, Adam).  \n",
      "- **Training / Validation / Test split**: Partitioning data to fit the model, tune hyper‑parameters, and evaluate final performance.  \n",
      "- **Overfitting / Underfitting**: Overfitting = model fits training data too closely and fails to generalize; underfitting = model is too simple to capture underlying patterns.  \n",
      "- **Regularization**: Techniques (e.g., L1/L2 penalties, dropout) that discourage overfitting.  \n",
      "- **Hyper‑parameters**: Settings not learned from data (e.g., learning rate, tree depth) that must be chosen manually or via search.  \n",
      "- **Evaluation metrics**: Accuracy, precision/recall, F1‑score, ROC‑AUC for classification; RMSE, MAE for regression; BLEU for translation, etc.\n",
      "\n",
      "---\n",
      "\n",
      "## Common Applications\n",
      "\n",
      "| Domain | Example Use Cases |\n",
      "|--------|-------------------|\n",
      "| **Computer vision** | Image classification, object detection, facial recognition, medical imaging analysis |\n",
      "| **Natural language processing (NLP)** | Sentiment analysis, machine translation, chatbots, summarization, speech recognition |\n",
      "| **Healthcare** | Predicting disease risk, drug discovery, personalized treatment recommendations |\n",
      "| **Finance** | Fraud detection, algorithmic trading, credit scoring, risk assessment |\n",
      "| **Recommendation systems** | Product/movie/music recommendations on e‑commerce or streaming platforms |\n",
      "| **Robotics & autonomous systems** | Navigation, grasping, control policies for self‑driving cars |\n",
      "| **Manufacturing** | Predictive maintenance, quality inspection, demand forecasting |\n",
      "| **Science & engineering** | Climate modeling, particle physics data analysis, materials design |\n",
      "\n",
      "---\n",
      "\n",
      "## Typical Workflow\n",
      "\n",
      "1. **Problem definition** – Clarify the objective, input/output format, and success criteria.  \n",
      "2. **Data collection** – Gather raw data from sensors, databases, APIs, etc.  \n",
      "3. **Data preprocessing** – Clean, normalize, handle missing values, encode categorical variables, augment data if needed.  \n",
      "4. **Feature engineering** – Create informative features (or let deep models learn them automatically).  \n",
      "5. **Model selection** – Choose a family of algorithms appropriate for the task and data size.  \n",
      "6. **Training** – Fit the model on the training set, often using cross‑validation to tune hyper‑parameters.  \n",
      "7. **Evaluation** – Measure performance on a held‑out test set using relevant metrics.  \n",
      "8. **Deployment** – Export the model (e.g., as a REST API, embedded device) and monitor its behavior in production.  \n",
      "9. **Maintenance** – Retrain or update the model as data distribution shifts (concept drift).\n",
      "\n",
      "---\n",
      "\n",
      "## Why Machine Learning Matters\n",
      "\n",
      "- **Scalability**: Automates tasks that would be infeasible to program manually for massive, high‑dimensional data.  \n",
      "- **Adaptability**: Models can be updated as new data arrives, allowing systems to evolve.  \n",
      "- **Insight extraction**: Uncovers hidden patterns, relationships, or predictive signals that human analysts might miss.  \n",
      "- **Economic impact**: Drives efficiencies, new products, and services across virtually every industry.\n",
      "\n",
      "---\n",
      "\n",
      "## Getting Started Resources\n",
      "\n",
      "- **Books**  \n",
      "  - *Pattern Recognition and Machine Learning* – Christopher Bishop  \n",
      "  - *Hands‑On Machine Learning with Scikit‑Learn, Keras, and TensorFlow* – Aurélien Géron  \n",
      "\n",
      "- **Online Courses**  \n",
      "  - Andrew Ng’s *Machine Learning* (Coursera) – classic intro with Octave/Matlab.  \n",
      "  - *Deep Learning Specialization* (Coursera) – modern neural network techniques.  \n",
      "  - Fast.ai’s *Practical Deep Learning for Coders* – code‑first approach.  \n",
      "\n",
      "- **Frameworks & Libraries**  \n",
      "  - **Python**: scikit‑learn (classical ML), TensorFlow / PyTorch (deep learning).  \n",
      "  - **R**: caret, mlr, tidymodels.  \n",
      "  - **Julia**: Flux, MLJ.  \n",
      "\n",
      "- **Communities**  \n",
      "  - Stack Overflow, Cross‑Validated (stats), Reddit r/MachineLearning, Kaggle (competitions & datasets).\n",
      "\n",
      "---\n",
      "\n",
      "### TL;DR\n",
      "\n",
      "Machine learning is the discipline of building algorithms that *learn* from data to make predictions or discover structure, without being explicitly programmed for each specific task. It encompasses a variety of methods (supervised, unsupervised, reinforcement, etc.), relies on data, models, and optimization, and powers many modern technologies from image search to autonomous vehicles.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is machine learning',\n",
       " 'output': '**Machine learning (ML)** is a sub‑field of artificial intelligence (AI) that focuses on developing algorithms and statistical models that enable computers to *learn* from data, rather than being explicitly programmed for every task. In other words, instead of writing a set of hard‑coded rules, we give a system examples (training data) and let it infer the underlying patterns, relationships, or decision boundaries that it can then apply to new, unseen data.\\n\\n---\\n\\n## Core Idea\\n\\n1. **Data → Model → Prediction**  \\n   - **Data**: A collection of examples (e.g., images, text, sensor readings) often accompanied by labels (e.g., “cat” vs. “dog”).  \\n   - **Model**: A parameterized mathematical function (e.g., a linear equation, a neural network) that maps inputs to outputs.  \\n   - **Learning**: An optimization process that adjusts the model’s parameters to minimize a loss (error) on the training data.  \\n   - **Prediction/Inference**: Once trained, the model can generate outputs for new inputs.\\n\\n2. **Generalization**  \\n   The ultimate goal is *generalization*: the ability of the learned model to perform well on data it has never seen before.\\n\\n---\\n\\n## Main Types of Machine Learning\\n\\n| Category | How it works | Typical tasks | Example algorithms |\\n|----------|--------------|---------------|--------------------|\\n| **Supervised learning** | Learns a mapping from inputs **x** to known outputs **y** using labeled examples. | Classification, regression, object detection | Linear regression, logistic regression, decision trees, support‑vector machines, neural networks |\\n| **Unsupervised learning** | Discovers structure in data without explicit labels. | Clustering, dimensionality reduction, anomaly detection | k‑means, hierarchical clustering, Gaussian mixture models, PCA, autoencoders |\\n| **Semi‑supervised learning** | Combines a small set of labeled data with a larger set of unlabeled data. | Improves performance when labeling is expensive | Self‑training, graph‑based methods, consistency regularization |\\n| **Self‑supervised learning** | Generates pseudo‑labels from the data itself (e.g., predicting missing parts). | Pre‑training representations for downstream tasks | Contrastive learning, masked language modeling (BERT) |\\n| **Reinforcement learning (RL)** | An agent learns by interacting with an environment and receiving reward signals. | Game playing, robotics, resource allocation | Q‑learning, policy gradients, Deep Q‑Network (DQN), Proximal Policy Optimization (PPO) |\\n| **Online / incremental learning** | Updates the model continuously as new data arrives. | Real‑time recommendation, streaming analytics | Stochastic gradient descent, online bagging |\\n| **Transfer learning** | Reuses knowledge from one task/domain to accelerate learning on another. | Fine‑tuning a pretrained image classifier on a new dataset | Pre‑trained CNNs (e.g., ResNet), language models (e.g., GPT) |\\n\\n---\\n\\n## Key Concepts & Terminology\\n\\n- **Feature**: A measurable property or representation of the input (e.g., pixel intensity, word frequency).  \\n- **Label / Target**: The ground‑truth output the model should predict (e.g., “spam” vs. “not spam”).  \\n- **Loss function**: Quantifies how far the model’s predictions are from the true labels (e.g., mean squared error, cross‑entropy).  \\n- **Optimization algorithm**: Adjusts model parameters to minimize the loss (e.g., gradient descent, Adam).  \\n- **Training / Validation / Test split**: Partitioning data to fit the model, tune hyper‑parameters, and evaluate final performance.  \\n- **Overfitting / Underfitting**: Overfitting = model fits training data too closely and fails to generalize; underfitting = model is too simple to capture underlying patterns.  \\n- **Regularization**: Techniques (e.g., L1/L2 penalties, dropout) that discourage overfitting.  \\n- **Hyper‑parameters**: Settings not learned from data (e.g., learning rate, tree depth) that must be chosen manually or via search.  \\n- **Evaluation metrics**: Accuracy, precision/recall, F1‑score, ROC‑AUC for classification; RMSE, MAE for regression; BLEU for translation, etc.\\n\\n---\\n\\n## Common Applications\\n\\n| Domain | Example Use Cases |\\n|--------|-------------------|\\n| **Computer vision** | Image classification, object detection, facial recognition, medical imaging analysis |\\n| **Natural language processing (NLP)** | Sentiment analysis, machine translation, chatbots, summarization, speech recognition |\\n| **Healthcare** | Predicting disease risk, drug discovery, personalized treatment recommendations |\\n| **Finance** | Fraud detection, algorithmic trading, credit scoring, risk assessment |\\n| **Recommendation systems** | Product/movie/music recommendations on e‑commerce or streaming platforms |\\n| **Robotics & autonomous systems** | Navigation, grasping, control policies for self‑driving cars |\\n| **Manufacturing** | Predictive maintenance, quality inspection, demand forecasting |\\n| **Science & engineering** | Climate modeling, particle physics data analysis, materials design |\\n\\n---\\n\\n## Typical Workflow\\n\\n1. **Problem definition** – Clarify the objective, input/output format, and success criteria.  \\n2. **Data collection** – Gather raw data from sensors, databases, APIs, etc.  \\n3. **Data preprocessing** – Clean, normalize, handle missing values, encode categorical variables, augment data if needed.  \\n4. **Feature engineering** – Create informative features (or let deep models learn them automatically).  \\n5. **Model selection** – Choose a family of algorithms appropriate for the task and data size.  \\n6. **Training** – Fit the model on the training set, often using cross‑validation to tune hyper‑parameters.  \\n7. **Evaluation** – Measure performance on a held‑out test set using relevant metrics.  \\n8. **Deployment** – Export the model (e.g., as a REST API, embedded device) and monitor its behavior in production.  \\n9. **Maintenance** – Retrain or update the model as data distribution shifts (concept drift).\\n\\n---\\n\\n## Why Machine Learning Matters\\n\\n- **Scalability**: Automates tasks that would be infeasible to program manually for massive, high‑dimensional data.  \\n- **Adaptability**: Models can be updated as new data arrives, allowing systems to evolve.  \\n- **Insight extraction**: Uncovers hidden patterns, relationships, or predictive signals that human analysts might miss.  \\n- **Economic impact**: Drives efficiencies, new products, and services across virtually every industry.\\n\\n---\\n\\n## Getting Started Resources\\n\\n- **Books**  \\n  - *Pattern Recognition and Machine Learning* – Christopher Bishop  \\n  - *Hands‑On Machine Learning with Scikit‑Learn, Keras, and TensorFlow* – Aurélien Géron  \\n\\n- **Online Courses**  \\n  - Andrew Ng’s *Machine Learning* (Coursera) – classic intro with Octave/Matlab.  \\n  - *Deep Learning Specialization* (Coursera) – modern neural network techniques.  \\n  - Fast.ai’s *Practical Deep Learning for Coders* – code‑first approach.  \\n\\n- **Frameworks & Libraries**  \\n  - **Python**: scikit‑learn (classical ML), TensorFlow / PyTorch (deep learning).  \\n  - **R**: caret, mlr, tidymodels.  \\n  - **Julia**: Flux, MLJ.  \\n\\n- **Communities**  \\n  - Stack Overflow, Cross‑Validated (stats), Reddit r/MachineLearning, Kaggle (competitions & datasets).\\n\\n---\\n\\n### TL;DR\\n\\nMachine learning is the discipline of building algorithms that *learn* from data to make predictions or discover structure, without being explicitly programmed for each specific task. It encompasses a variety of methods (supervised, unsupervised, reinforcement, etc.), relies on data, models, and optimization, and powers many modern technologies from image search to autonomous vehicles.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"What is machine learning\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `arxiv` with `{'query': '1706.03762'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPublished: 2023-08-02\n",
      "Title: Attention Is All You Need\n",
      "Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
      "Summary: The dominant sequence transduction models are based on c\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'Attention Is All You Need'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Attention Is All You Need\n",
      "Summary: \"Attention Is All You Need\" is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer,\u001b[0m\u001b[32;1m\u001b[1;3m**Paper:** *Attention Is All You Need* (arXiv:1706.03762)  \n",
      "**Authors:** Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin  \n",
      "**Published:** June 2017 (Conference: NIPS 2017)  \n",
      "\n",
      "---\n",
      "\n",
      "### 1. Core Idea\n",
      "The paper introduces the **Transformer** architecture—a neural network for sequence‑to‑sequence tasks that **relies solely on attention mechanisms** and discards recurrent (RNN/LSTM) and convolutional layers. By doing so, it enables **full parallelism** over input and output positions, dramatically speeding up training and inference while achieving superior performance.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Architecture Overview  \n",
      "\n",
      "| Component | Description |\n",
      "|-----------|-------------|\n",
      "| **Multi‑Head Self‑Attention** | Each token attends to every other token in the same sequence, using several parallel attention “heads” to capture different relational patterns. |\n",
      "| **Positional Encoding** | Since attention alone is order‑agnostic, sinusoidal (or learned) positional encodings are added to token embeddings to inject sequence order information. |\n",
      "| **Feed‑Forward Networks (FFN)** | Position‑wise fully‑connected layers applied independently to each token after attention. |\n",
      "| **Encoder‑Decoder Stack** | The model stacks *N* identical layers (the paper uses *N = 6*) for both encoder and decoder. The decoder’s self‑attention is masked to prevent attending to future positions. |\n",
      "| **Residual Connections + Layer Normalization** | Each sub‑layer (attention, FFN) is wrapped with a residual connection followed by layer norm, stabilizing training. |\n",
      "| **Output Projection + Softmax** | Decoder outputs are linearly projected to the target vocabulary size and passed through a softmax to produce probability distributions over tokens. |\n",
      "\n",
      "The overall computational complexity per layer is **O(n²·d)** for sequence length *n* and model dimension *d*, but because all positions are processed in parallel, wall‑clock time is far lower than recurrent models.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Training Details  \n",
      "\n",
      "* **Datasets:** WMT 2014 English‑German (≈4.5 M sentence pairs) and English‑French (≈36 M sentence pairs).  \n",
      "* **Optimization:** Adam optimizer with β₁=0.9, β₂=0.98, ε=10⁻⁹; learning rate follows a warm‑up schedule:  \n",
      "  \\[\n",
      "  \\text{lr}=d_{\\text{model}}^{-0.5}\\,\\min(\\text{step}^{-0.5},\\ \\text{step}\\cdot \\text{warmup}^{-1.5})\n",
      "  \\]  \n",
      "* **Regularization:** Dropout (0.1) on attention weights and FFN layers; label smoothing (ε=0.1).  \n",
      "* **Model Size:** Base model – d_model = 512, d_ff = 2048, 8 attention heads, ~65 M parameters. A larger “Transformer‑Big” variant is also explored.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Results  \n",
      "\n",
      "| Task | Metric | Transformer (Base) | Prior SOTA (e.g., ConvS2S, RNMT+) |\n",
      "|------|--------|--------------------|----------------------------------|\n",
      "| **EN→DE** (BLEU) | 28.4 | **28.4** (tied) | 26.3 (ConvS2S) |\n",
      "| **EN→FR** (BLEU) | 41.0 | **41.0** (tied) | 39.2 (RNMT+) |\n",
      "\n",
      "*Training time* for the base model on 8 GPUs is **≈3.5 days**, roughly **3× faster** than comparable recurrent baselines.\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Key Contributions  \n",
      "\n",
      "1. **Self‑Attention as a Primitive:** Demonstrated that attention alone can model dependencies across arbitrary distances without recurrence or convolution.  \n",
      "2. **Parallelizable Architecture:** By eliminating sequential dependencies, the Transformer enables massive parallelism, reducing training time dramatically.  \n",
      "3. **Multi‑Head Mechanism:** Allows the model to jointly attend to information from different representation subspaces.  \n",
      "4. **Positional Encoding Scheme:** Simple sinusoidal encodings that let the model learn relative and absolute positions.  \n",
      "5. **State‑of‑the‑Art Performance:** Set new BLEU scores on major machine‑translation benchmarks while being computationally more efficient.\n",
      "\n",
      "---\n",
      "\n",
      "### 6. Impact & Follow‑Up Work  \n",
      "\n",
      "The Transformer has become the **foundational architecture** for modern NLP and many other domains:\n",
      "\n",
      "* **Pre‑training models** – BERT, GPT, RoBERTa, T5, etc., all build on the encoder or decoder stack.  \n",
      "* **Vision Transformers (ViT)** – applying self‑attention to image patches.  \n",
      "* **Speech, Reinforcement Learning, Graphs** – numerous adaptations across modalities.  \n",
      "\n",
      "Its introduction sparked an entire research ecosystem focused on scaling attention, improving efficiency (e.g., Longformer, Performer, Reformer), and exploring architectural variants.\n",
      "\n",
      "---\n",
      "\n",
      "### 7. TL;DR Summary  \n",
      "\n",
      "*The paper proposes the Transformer, a sequence‑to‑sequence model that replaces recurrence with multi‑head self‑attention, enabling full parallel processing of sequences. It achieves better translation quality (BLEU) than previous RNN/conv‑based models while training significantly faster, and it has since become the backbone of virtually all large‑scale language models.*\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"What's the paper 1706.03762 about?\",\n",
       " 'output': '**Paper:** *Attention Is All You Need* (arXiv:1706.03762)  \\n**Authors:** Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\\u202fGomez, Łukasz\\u202fKaiser, Illia\\u202fPolosukhin  \\n**Published:** June\\u202f2017 (Conference: NIPS 2017)  \\n\\n---\\n\\n### 1. Core Idea\\nThe paper introduces the **Transformer** architecture—a neural network for sequence‑to‑sequence tasks that **relies solely on attention mechanisms** and discards recurrent (RNN/LSTM) and convolutional layers. By doing so, it enables **full parallelism** over input and output positions, dramatically speeding up training and inference while achieving superior performance.\\n\\n---\\n\\n### 2. Architecture Overview  \\n\\n| Component | Description |\\n|-----------|-------------|\\n| **Multi‑Head Self‑Attention** | Each token attends to every other token in the same sequence, using several parallel attention “heads” to capture different relational patterns. |\\n| **Positional Encoding** | Since attention alone is order‑agnostic, sinusoidal (or learned) positional encodings are added to token embeddings to inject sequence order information. |\\n| **Feed‑Forward Networks (FFN)** | Position‑wise fully‑connected layers applied independently to each token after attention. |\\n| **Encoder‑Decoder Stack** | The model stacks *N* identical layers (the paper uses *N = 6*) for both encoder and decoder. The decoder’s self‑attention is masked to prevent attending to future positions. |\\n| **Residual Connections + Layer Normalization** | Each sub‑layer (attention, FFN) is wrapped with a residual connection followed by layer norm, stabilizing training. |\\n| **Output Projection + Softmax** | Decoder outputs are linearly projected to the target vocabulary size and passed through a softmax to produce probability distributions over tokens. |\\n\\nThe overall computational complexity per layer is **O(n²·d)** for sequence length *n* and model dimension *d*, but because all positions are processed in parallel, wall‑clock time is far lower than recurrent models.\\n\\n---\\n\\n### 3. Training Details  \\n\\n* **Datasets:** WMT\\u202f2014 English‑German (≈4.5\\u202fM sentence pairs) and English‑French (≈36\\u202fM sentence pairs).  \\n* **Optimization:** Adam optimizer with β₁=0.9, β₂=0.98, ε=10⁻⁹; learning rate follows a warm‑up schedule:  \\n  \\\\[\\n  \\\\text{lr}=d_{\\\\text{model}}^{-0.5}\\\\,\\\\min(\\\\text{step}^{-0.5},\\\\ \\\\text{step}\\\\cdot \\\\text{warmup}^{-1.5})\\n  \\\\]  \\n* **Regularization:** Dropout (0.1) on attention weights and FFN layers; label smoothing (ε=0.1).  \\n* **Model Size:** Base model – d_model\\u202f=\\u202f512, d_ff\\u202f=\\u202f2048, 8 attention heads, ~65\\u202fM parameters. A larger “Transformer‑Big” variant is also explored.\\n\\n---\\n\\n### 4. Results  \\n\\n| Task | Metric | Transformer (Base) | Prior SOTA (e.g., ConvS2S, RNMT+) |\\n|------|--------|--------------------|----------------------------------|\\n| **EN→DE** (BLEU) | 28.4 | **28.4** (tied) | 26.3 (ConvS2S) |\\n| **EN→FR** (BLEU) | 41.0 | **41.0** (tied) | 39.2 (RNMT+) |\\n\\n*Training time* for the base model on 8 GPUs is **≈3.5 days**, roughly **3× faster** than comparable recurrent baselines.\\n\\n---\\n\\n### 5. Key Contributions  \\n\\n1. **Self‑Attention as a Primitive:** Demonstrated that attention alone can model dependencies across arbitrary distances without recurrence or convolution.  \\n2. **Parallelizable Architecture:** By eliminating sequential dependencies, the Transformer enables massive parallelism, reducing training time dramatically.  \\n3. **Multi‑Head Mechanism:** Allows the model to jointly attend to information from different representation subspaces.  \\n4. **Positional Encoding Scheme:** Simple sinusoidal encodings that let the model learn relative and absolute positions.  \\n5. **State‑of‑the‑Art Performance:** Set new BLEU scores on major machine‑translation benchmarks while being computationally more efficient.\\n\\n---\\n\\n### 6. Impact & Follow‑Up Work  \\n\\nThe Transformer has become the **foundational architecture** for modern NLP and many other domains:\\n\\n* **Pre‑training models** – BERT, GPT, RoBERTa, T5, etc., all build on the encoder or decoder stack.  \\n* **Vision Transformers (ViT)** – applying self‑attention to image patches.  \\n* **Speech, Reinforcement Learning, Graphs** – numerous adaptations across modalities.  \\n\\nIts introduction sparked an entire research ecosystem focused on scaling attention, improving efficiency (e.g., Longformer, Performer, Reformer), and exploring architectural variants.\\n\\n---\\n\\n### 7. TL;DR Summary  \\n\\n*The paper proposes the Transformer, a sequence‑to‑sequence model that replaces recurrence with multi‑head self‑attention, enabling full parallel processing of sequences. It achieves better translation quality (BLEU) than previous RNN/conv‑based models while training significantly faster, and it has since become the backbone of virtually all large‑scale language models.*'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"What's the paper 1706.03762 about?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
